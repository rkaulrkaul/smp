{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science final project on Climate Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import pandas\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "# from selenium import webdriver\n",
    "import time\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Question: \n",
    "Is climate change affecting the rate and strength of storms?\n",
    "\n",
    "Our goal is to compare and contrast anomalies in a climate with the variables such as damage from storms, and tornado intensity over the years.\n",
    "\n",
    "Dataset 1:\n",
    "National Centers for Environmental Information - National Weather Service (NWS).\n",
    "Storm Events Database contains data from January 1950 to January 2019.\n",
    "https://www.ncdc.noaa.gov/stormevents/ftp.jsp\n",
    "\n",
    "The NWS split up data of all storm events into 69 files based on year. We extracted six variables from the data and joined all of the data into a single PANDAS data frame. Variables of interest include storm year, damage to property, damage to crops, direct deaths, direct injuries, and tornado intensity based on the Fujita-Pearson scale. Currently, this dataset has over 1.5 million rows of data. For analysis, a lot of this data will be consolidated into single year rows rather than multiple rows with the same year. Columns such as tornado intensity (F1, F2, …, F5) will need to be transformed into integers for easier calculations.\n",
    "\n",
    "Dataset 2:\n",
    "Data collected from GISS Surface Temperature (GISTEMP) and Climate at a Glance (GCAG) by DataHub. Contains data from 1880 to 2016.\n",
    "https://datahub.io/core/global-temp\n",
    "\n",
    "We have two variables for this set- year, and average global mean anomalies for temperature in Celsius. The base period for temperature, from where they obtained their mean is from 1951 to 1980. The second variable measures the difference from this mean, with negative being colder and positive being warmer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\coolm\\\\Desktop\\\\2950 final project\\\\data\\\\storm-data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-539730d0b9f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstorm_current_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\data\\storm-data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstorm_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorm_current_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m## put all csv into one pandas framework\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwanted_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"YEAR\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DAMAGE_PROPERTY'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DEATHS_DIRECT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DAMAGE_CROPS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"STATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\coolm\\\\Desktop\\\\2950 final project\\\\data\\\\storm-data'"
     ]
    }
   ],
   "source": [
    "storm_current_dir = os.getcwd()+'\\data\\storm-data'\n",
    "storm_files = os.listdir(storm_current_dir)\n",
    "\n",
    "## put all csv into one pandas framework \n",
    "wanted_columns = [\"YEAR\",'DAMAGE_PROPERTY', 'DEATHS_DIRECT', 'DAMAGE_CROPS', \"STATE\"]\n",
    "\n",
    "##list of data frames\n",
    "list_df = []\n",
    "list_damage_property = []\n",
    "list_deaths_direct = []\n",
    "list_damage_crops = []\n",
    "list_injuries_direct = []\n",
    "list_tor_scale = []\n",
    "list_events = []\n",
    "state_with_most_damage = []\n",
    "for file in storm_files:\n",
    "    df = pandas.read_csv(storm_current_dir+\"\\/\"+file, low_memory=False, dtype={})[wanted_columns]\n",
    "    list_df.append(df)\n",
    "    list_damage_property.append(df[[\"YEAR\",\"DAMAGE_PROPERTY\"]])\n",
    "    list_deaths_direct.append(df[[\"YEAR\", \"DEATHS_DIRECT\"]])\n",
    "    list_damage_crops.append(df[[\"YEAR\", \"DAMAGE_CROPS\"]])\n",
    "    list_events.append(df[[\"YEAR\"]])\n",
    "    state_with_most_damage.append(df[[\"YEAR\", \"STATE\", \"DAMAGE_PROPERTY\"]])\n",
    "    \n",
    "## Concat lists\n",
    "storm_dataframe = pandas.concat(list_df)\n",
    "damage_property = pandas.concat(list_damage_property)\n",
    "deaths_direct = pandas.concat(list_deaths_direct)\n",
    "damage_crops = pandas.concat(list_damage_crops)\n",
    "events = pandas.concat(list_events)\n",
    "state_with_most_damage = pandas.concat(state_with_most_damage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph below shows the estimated amount of damage to property incurred by the weather event amout of damage in dollars over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Linear Regressions for each dataset\n",
    "#Damage to Property\n",
    "#Property Damage\n",
    "damage_property = damage_property[pandas.notnull(damage_property[\"DAMAGE_PROPERTY\"])]\n",
    "damage_property = damage_property[~damage_property[\"DAMAGE_PROPERTY\"].str.contains(\"H\")]\n",
    "damage_property = damage_property[~damage_property[\"DAMAGE_PROPERTY\"].str.contains(\"h\")]\n",
    "damage_property['DAMAGE_PROPERTY'] = damage_property['DAMAGE_PROPERTY'].str.extract(r'[\\d\\.]+([KMBTkmbt]+)', expand=False).fillna(1).replace(['K','M','B','T','k','m','b','t'], [10**3, 10**6, 10**9, 10**12,10**3, 10**6, 10**9, 10**12]).astype(int) * pandas.to_numeric(damage_property['DAMAGE_PROPERTY'].replace(r'[KMBTkmbt]+$', '', regex=True))\n",
    "\n",
    "pyplot.scatter(damage_property[\"YEAR\"], damage_property[\"DAMAGE_PROPERTY\"], c = \"red\", alpha = 0.2)\n",
    "pyplot.show()\n",
    "linregress(damage_property['YEAR'],damage_property['DAMAGE_PROPERTY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The number of deaths per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Deaths from storms\n",
    "deaths_direct = deaths_direct[pandas.notnull(deaths_direct[\"DEATHS_DIRECT\"])]\n",
    "pyplot.scatter(deaths_direct[\"YEAR\"], deaths_direct[\"DEATHS_DIRECT\"], c = \"red\", alpha = 0.1)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph below shows the the estimated amount of damage to crops incurred by the weather event over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damage to Crops\n",
    "# Damage to crops, K (x 1000) removed\n",
    "damage_crops = damage_crops[pandas.notnull(damage_crops[\"DAMAGE_CROPS\"])]\n",
    "damage_crops[\"DAMAGE_CROPS\"] = damage_crops[\"DAMAGE_CROPS\"].apply(str).str.replace(\"?\",\"\")\n",
    "a = damage_crops['DAMAGE_CROPS'].str.extract(r'[\\d\\.]+([KkMmBbTt]+)', expand=False).fillna(1).replace(['K','k','m','M','b','B','t','T'], [10**3,10**3, 10**6,10**6, 10**9,10**9,10**12,10**12]).astype(int)\n",
    "b = pandas.to_numeric(damage_crops['DAMAGE_CROPS'].replace(r'[KkMmBbtT]+$', '', regex=True))\n",
    "damage_crops['DAMAGE_CROPS'] =  a*b\n",
    "\n",
    "pyplot.scatter(damage_crops[\"YEAR\"], damage_crops[\"DAMAGE_CROPS\"], c = \"blue\", alpha = 0.2)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph below shows the relationship between the number of injuries directly related to the weather event and years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Injuries\n",
    "injuries_direct = injuries_direct[pandas.notnull(injuries_direct[\"INJURIES_DIRECT\"])]\n",
    "\n",
    "pyplot.scatter(injuries_direct[\"YEAR\"], injuries_direct[\"INJURIES_DIRECT\"], c = \"purple\", alpha = 0.08)\n",
    "pyplot.show()\n",
    "\n",
    "print(\"-------------statistics-------------\")\n",
    "linregress(injuries_direct['YEAR'],injuries_direct['INJURIES_DIRECT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph below shows the number storms that occur  every year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storms per year\n",
    "counts = events[\"YEAR\"].value_counts()\n",
    "pyplot.scatter(counts.index, counts)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph below shows the number storms that occur  every year (excluding  2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storms per year without incomplete 2019 point\n",
    "counts_2018 = counts[counts.index != 2019]\n",
    "pyplot.scatter(counts_2018.index, counts_2018)\n",
    "pyplot.show()\n",
    "linregress(counts_2018.index,counts_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------statistics-------------\")\n",
    "linregress(counts_2018.index,counts_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_attributes = [[damage_property, 'DAMAGE_PROPERTY'], [deaths_direct,'DEATHS_DIRECT'], [damage_crops,'DAMAGE_CROPS']]\n",
    "for attribute in stats_attributes:\n",
    "    print('\\n',attribute[1])\n",
    "    print(\"Mean:\", attribute[0][attribute[1]].mean())\n",
    "    print(\"Median:\", attribute[0][attribute[1]].median())\n",
    "    print(\"Standard Deviation:\", attribute[0][attribute[1]].std())\n",
    "print(\"\\nSTORM_COUNT\")\n",
    "print(\"Mean:\", counts_2018.mean())\n",
    "print(\"Median:\", counts_2018.median())\n",
    "print(\"Standard Deviation:\", counts_2018.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which states have the highst damamge rates by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_with_most_damage = state_with_most_damage[pandas.notnull(state_with_most_damage[\"DAMAGE_PROPERTY\"])]\n",
    "state_with_most_damage = state_with_most_damage[~state_with_most_damage[\"DAMAGE_PROPERTY\"].str.contains(\"H\")]\n",
    "state_with_most_damage = state_with_most_damage[~state_with_most_damage[\"DAMAGE_PROPERTY\"].str.contains(\"h\")]\n",
    "state_with_most_damage['DAMAGE_PROPERTY'] = state_with_most_damage['DAMAGE_PROPERTY'].str.extract(r'[\\d\\.]+([KMBTkmbt]+)', expand=False).fillna(1).replace(['K','M','B','T','k','m','b','t'], [10**3, 10**6, 10**9, 10**12,10**3, 10**6, 10**9, 10**12]).astype(int) * pandas.to_numeric(state_with_most_damage['DAMAGE_PROPERTY'].replace(r'[KMBTkmbt]+$', '', regex=True))\n",
    "state_with_most_damage = state_with_most_damage[state_with_most_damage['STATE'] != \"ALASKA\"]\n",
    "state_with_most_damage = state_with_most_damage[state_with_most_damage['STATE'] != \"HAWAII\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make dict structure like so: year: [damage, state]\n",
    "state_with_most_dam_by_year = {}\n",
    "for row in state_with_most_damage.iterrows():\n",
    "    year = row[1][0]\n",
    "    state = row[1][1]\n",
    "    damage = row[1][2]\n",
    "    if year in state_with_most_dam_by_year:\n",
    "        last_data = state_with_most_dam_by_year[year]\n",
    "        if damage > last_data[0]:\n",
    "            last_data[0] = damage\n",
    "            last_data[1] = state\n",
    "    else:\n",
    "        state_with_most_dam_by_year[int(year)] = [damage, state]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record high damage from each year caused by storms\n",
    "\n",
    "I added up the damage caused by each state and year and from that list took the highest damage from each year and plotted  this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "damage = [item[0] for item in list(state_with_most_dam_by_year.values())]\n",
    "years = [item for item in list(state_with_most_dam_by_year.keys())]\n",
    "\n",
    "pyplot.figure(figsize=(35,10))\n",
    "pyplot.scatter(years[:-1], damage[:-1], c = \"orange\")\n",
    "pyplot.ticklabel_format(style='plain', axis='y')\n",
    "pyplot.show()\n",
    "\n",
    "print(\"---------statistics---------\")\n",
    "print(linregress(years,damage))\n",
    "print(\"Coefficient of Determination\",linregress(years,damage)[2] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting temperature data by state by year (base line temp is from 1901-2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for state in range(1,51):\n",
    "#     url = \"https://www.ncdc.noaa.gov/cag/statewide/time-series/{}-tavg-12-12-1895-2019.csv?base_prd=true&begbaseyear=1901&endbaseyear=2000\".format(state)\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#     file = open(\"data/temperature_data/by_state/{}.csv\".format(state), \"w\")\n",
    "#     file.write(soup.prettify())\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################################temperature data by state by year####################################################\n",
    "def split_it(year):\n",
    "    return int((re.search('^\\d\\d\\d\\d', year)).group(0))\n",
    "\n",
    "states_avg_tem = os.getcwd()+r'\\data\\temperature_data\\by_state'\n",
    "states_avg_tem_files = os.listdir(states_avg_tem)\n",
    "\n",
    "avg_temp_by_state = []\n",
    "count = 0\n",
    "for file in states_avg_tem_files:\n",
    "    state_file = pandas.read_csv(r\"data\\temperature_data\\by_state\\\\\"+file, skiprows = [1,2,3])\n",
    "    temp = state_file.columns\n",
    "    state_file['state'] = temp[0]\n",
    "    state_file.columns = [\"Date\",\"Value\",\"Anomaly\",\"State\"]\n",
    "    state_file = state_file.drop(state_file.index[0])\n",
    "    state_file['Date'] = state_file['Date'].apply(split_it)\n",
    "    state_file['Anomaly'] = pandas.to_numeric(state_file['Anomaly'])\n",
    "    avg_temp_by_state.append(state_file[[\"Date\",\"Value\",\"Anomaly\",\"State\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp_df = pandas.concat(avg_temp_by_state)\n",
    "temp_df = temp_df[temp_df['State'].str.lower() != \"hawaii\"]\n",
    "temp_df = temp_df[temp_df['State'].str.lower() != \"alaska\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_anomaly_by_state = {}\n",
    "for row in temp_df.iterrows():\n",
    "    if row[1][0] in total_anomaly_by_state:\n",
    "        current_anom = total_anomaly_by_state[row[1][0]][0]\n",
    "        if row[1][2] > current_anom:\n",
    "            total_anomaly_by_state[row[1][0]] = [row[1][2], row[1][3]] \n",
    "    else:\n",
    "        total_anomaly_by_state[row[1][0]] = [row[1][2], row[1][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure that they both have the same number of years\n",
    "for k in total_anomaly_by_state.copy():\n",
    "    if k < 1950 or k >2018:\n",
    "        total_anomaly_by_state.pop(k, None)\n",
    "        \n",
    "for k in state_with_most_dam_by_year.copy():\n",
    "    if k < 1950 or k >2018:\n",
    "        state_with_most_dam_by_year.pop(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## browser automation to grab the distance_from_to\n",
    "# state_orders = []\n",
    "# chromedriver = \"C:\\chromedriver\"\n",
    "# browser = webdriver.Chrome(executable_path=chromedriver)\n",
    "# for index in range(1950, 2018):\n",
    "#     state_1 = state_with_most_dam_by_year[index][1].lower()\n",
    "#     state_2 = total_anomaly_by_state[index][1].lower()\n",
    "    \n",
    "#     ## calc the Straight line distance between the states\n",
    "#     url = \"https://www.mapdevelopers.com/distance_from_to.php?&from={}&to={}\".format(state_1.replace(\" \", \"%\"), state_2.replace(\" \", \"%\"))\n",
    "#     browser.get(url)\n",
    "#     time.sleep(3)\n",
    "#     html = browser.page_source\n",
    "#     soup = BeautifulSoup(html, 'html')\n",
    "#     distance = soup.find('div', {'id': 'status'}).text\n",
    "#     km = [int(s) for s in distance.split() if s.isdigit()]\n",
    "#     state_orders.append([km[0], index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The distance between states which have been calculated to be most damaged and had the most change in temperature in ascending order by year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # state_orders = [[4305416, 1950], [6336642, 1951], [6638793, 1952], [847466, 1953], [5251054, 1954], [4678393, 1955], [3246293, 1956], [7931992, 1957], [8206916, 1958], [7989981, 1959], [2057314, 1960], [5957785, 1961], [3563955, 1962], [6600911, 1963], [6079953, 1964], [4678393, 1965], [8138256, 1966], [8459949, 1967], [10177682, 1968], [8138256, 1969], [6126621, 1970], [3022415, 1971], [9520834, 1972], [6257021, 1973], [5643612, 1974], [7046574, 1975], [6369736, 1976], [6159076, 1977], [5980817, 1978], [1646095, 1979], [1299633, 1980], [4703559, 1981], [5300443, 1982], [8258023, 1983], [8520885, 1984], [2374460, 1985], [1825467, 1986], [6414853, 1987], [9426815, 1988], [11204921, 1989], [4496446, 1990], [6772869, 1991], [7322445, 1992], [5012759, 1993], [9903719, 1994], [4262659, 1995], [9581462, 1996], [6323297, 1997], [2507407, 1998], [4362950, 1999], [2329002, 2000], [6079953, 2001], [5851094, 2002], [2566425, 2003], [13293565, 2004], [5801403, 2005], [1856688, 2006], [4431277, 2007], [8055957, 2008], [5980817, 2009], [12246091, 2010], [4996171, 2011], [5886736, 2012], [7687942, 2013], [9703489, 2014], [7322445, 2015], [9139432, 2016], [3502657, 2017]]\n",
    "\n",
    "# x = [item[1] for item in state_orders]\n",
    "# y = [item[0] for item in state_orders]\n",
    "# pyplot.bar(x, y)\n",
    "# pyplot.show()\n",
    "\n",
    "# print(\"---------------statistics-------------------\")\n",
    "\n",
    "# linregress(years,damage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of storms events and Anomaly in temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years = temp_df[\"Date\"].value_counts()\n",
    "\n",
    "acc = []\n",
    "for index, value in all_years.items():\n",
    "    year_climate = temp_df.loc[temp_df[\"Date\"] == index]\n",
    "    average = pandas.to_numeric(year_climate[\"Anomaly\"]).mean()\n",
    "    acc.append([index, average])\n",
    "US_df =  pandas.DataFrame(acc, columns = [\"Year\", \"Average_Anomalies\"])\n",
    "counts_2018 =  pandas.DataFrame({'Year':counts_2018.index, 'Storm_Amount':counts_2018.values})\n",
    "US_df = US_df[pandas.to_numeric(US_df[\"Year\"]) > 1949]\n",
    "\n",
    "US_df[\"Year\"] = pandas.to_numeric(US_df[\"Year\"])\n",
    "combined = US_df.merge(counts_2018, on = [\"Year\"])\n",
    "\n",
    "pyplot.scatter(combined[\"Average_Anomalies\"], combined[\"Storm_Amount\"], c = \"orange\", alpha = 0.4)\n",
    "pyplot.xlabel(\"Anomaly in Temperature (°F)\")\n",
    "pyplot.ylabel(\"Number of Storm Events\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined.corr())\n",
    "print('-------------More statistics----------------')\n",
    "linresult = linregress(combined[\"Average_Anomalies\"],combined[\"Storm_Amount\"])\n",
    "print(linresult)\n",
    "print(\"Coefficient of Determination:\", linresult[2] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_crops = []\n",
    "for index, value in all_years.items():\n",
    "    year_dmg = damage_crops.loc[damage_crops[\"YEAR\"] == index]\n",
    "    total_dmg = year_dmg[\"DAMAGE_CROPS\"].astype(float).sum()\n",
    "    acc_crops.append([index, total_dmg])\n",
    "crops_frame = pandas.DataFrame(acc_crops, columns = [\"Year\", \"Total_Crop_Damage\"])\n",
    "combined_crops = US_df.merge(crops_frame, on = [\"Year\"])\n",
    "\n",
    "\n",
    "pyplot.scatter(combined_crops[\"Average_Anomalies\"], combined_crops[\"Total_Crop_Damage\"], c = \"green\", alpha = 0.4)\n",
    "pyplot.xlabel(\"Anomaly in Temperature (°F)\")\n",
    "pyplot.ylabel(\"Total Damage to Crops ($))\")\n",
    "pyplot.show()\n",
    "print(combined_crops.corr())\n",
    "print('\\n-------------More statistics----------------')\n",
    "linresult = linregress(combined_crops[\"Average_Anomalies\"],combined_crops[\"Total_Crop_Damage\"])\n",
    "print(linresult)\n",
    "print(\"Coefficient of Determination:\", linresult[2] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_property = []\n",
    "for index, value in all_years.items():\n",
    "    year_dmg = damage_property.loc[damage_property[\"YEAR\"] == index]\n",
    "    total_dmg = year_dmg[\"DAMAGE_PROPERTY\"].sum()\n",
    "    acc_property.append([index, total_dmg])\n",
    "property_frame = pandas.DataFrame(acc_property, columns = [\"Year\", \"Total_Property_Damage\"])\n",
    "combined_property = US_df.merge(property_frame, on = [\"Year\"])\n",
    "\n",
    "pyplot.scatter(combined_property[\"Average_Anomalies\"], combined_property[\"Total_Property_Damage\"], c = \"red\", alpha = 0.4)\n",
    "pyplot.xlabel(\"Anomaly in Temperature (°F)\")\n",
    "pyplot.ylabel(\"Total Damage to Property ($)\")\n",
    "pyplot.show()\n",
    "print(combined_property.corr())\n",
    "print('\\n-------------More statistics----------------')\n",
    "linresult = linregress(combined_property[\"Average_Anomalies\"],combined_property[\"Total_Property_Damage\"])\n",
    "print(linresult)\n",
    "print(\"Coefficient of Determination:\", linresult[2] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_deaths = []\n",
    "for index, value in all_years.items():\n",
    "    year_deaths = deaths_direct.loc[deaths_direct[\"YEAR\"] == index]\n",
    "    total_deaths = year_deaths[\"DEATHS_DIRECT\"].sum()\n",
    "    acc_deaths.append([index, total_deaths])\n",
    "deaths_df = pandas.DataFrame(acc_deaths, columns = [\"Year\", \"Total_Deaths\"])\n",
    "deaths_combined = US_df.merge(deaths_df, on = [\"Year\"])\n",
    "\n",
    "pyplot.scatter(deaths_combined[\"Average_Anomalies\"], deaths_combined[\"Total_Deaths\"], c = \"red\", alpha = 0.4)\n",
    "pyplot.xlabel(\"Anomaly in Temperature (°F)\")\n",
    "pyplot.ylabel(\"Direct Deaths\")\n",
    "pyplot.show()\n",
    "print(deaths_combined.corr())\n",
    "print('\\n-------------More statistics----------------')\n",
    "linresult = linregress(deaths_combined[\"Average_Anomalies\"],deaths_combined[\"Total_Deaths\"])\n",
    "print(linresult)\n",
    "print(\"Coefficient of Determination:\", linresult[2] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
